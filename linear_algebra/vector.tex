
\documentclass[12pt]{article}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\newtheorem{thm}{Theorem}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{indentfirst}
\setlength{\parindent}{0em}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{setspace}
\doublespacing
\usepackage[flushleft]{threeparttable}
\usepackage{booktabs,caption}
\usepackage{float}
\usepackage{graphicx}
\usepackage[sort,comma]{natbib}
\usepackage[hidelinks]{hyperref}

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\newcommand{\incfig}[1]{%
\def\svgwidth{\columnwidth}
\import{./figures/}{#1.pdf_tex}
}




\title{}
\author{}
\date{}


\begin{document}
\section{Orthogonal}

Column vectors $ \bm{a}, \bm{b} $, 
\begin{align*}
\bm{a} &= (a_1, a_2, \cdots, a_{n})'\\
\bm{b} &= (b_1, b_2, \cdots, b_{n})'
\end{align*}
if $ \bm{a}'\bm{b} = 0 $, they are orthogonal (they are perpendicular to each other.)
\begin{align*}
\bm{a}'\bm{b}&= (a_1, a_2, \cdots, a_{n})(b_1, b_2, \cdots, b_{n})'\\
&= a_1b_1 + a_2b_2 + \cdots + a_{n}b_{n}\\
&= \sum\limits_{i = 1} ^n a_{i}b_{i}	
\end{align*}

{\textbf {Special case:}}\\
If $ \bm{b} = \bm{a} $
\begin{equation*}
\bm{a}'\bm{a} = \sum\limits_{i = 1} ^n a_{i}^{2}	
\end{equation*}

{\textbf {Note:}}\\
$ \bm{a}'\bm{b} $ is called the {\textbf {inner product}} or the
{\textbf {dot product}} of vector $ \bm{a}  $ and $ \bm{b} $.







\section{Summation of Matrix}
Transpose
\begin{equation*}
(\bm{A} + \bm{B})' = \bm{A}' + \bm{B}'
\end{equation*}


\section{Production of Matrix}
\subsection{}
\begin{equation*}
\bm{AB} \ne \bm{BA}
\end{equation*}

\subsection{}
\begin{align*}
\bm{IA}&= \bm{AI} = \bm{A}\\
(\bm{AB})' &= \bm{B}'\bm{A}'\\
(\bm{ABC})' &= \bm{C}'\bm{B}'\bm{A}'
\end{align*}


\section{Invertible Matrix}
For matrix $ \bm{A} $, if there is a matrix $ \bm{B} $ that
$ \bm{AB} = \bm{BA} = \bm{I} $, 

then we say $ \bm{A} $ is invertible and it is a {\textbf {nonsingular matrix}}.
And we say $ \bm{B} $ is the {\textbf {inverse matrix}} of $ \bm{A} $, $ \bm{A}^{ - 1} $

{\textbf {Properties:}}
\begin{align*}
(\bm{A}^{ - 1})^{ - 1} &= \bm{A}\\
(\bm{AB})^{ - 1} &= \bm{B}^{ - 1}\bm{A}^{ - 1}\\
(\bm{ABC})^{ - 1}&= \bm{C}^{ - 1}\bm{B}^{ - 1}\bm{A}^{ - 1}
\end{align*}



\section{Linearly Dependent and Matrix Rank}
For two column vectors $ \bm{a_1}, \bm{a_2} $, if $ \bm{a_1} = k \bm{a_2} $, where
$ k $ is a constant, then only one vector contains useful information.

In a more general form, for $ K $ column vectors, $ \left\{ \bm{a_1}, \bm{a_2}, 
... \bm{a_{K}}\right\}  $, if there exists scalers $ c_1, ..., c_{K} $, not all of
them equal to zero, and they make
\begin{equation}
		\label{eqn:linear dependent}
		\sum\limits_{i = 1} ^K c_{i}\bm{a_{i}}	=0,
\end{equation}
then we say $ \left\{ \bm{a_1}, \bm{a_2}, ... \bm{a_{K}}\right\}  $ is 
{\textbf {linearly dependent}}.

It means there must be at least one vector can be written as a linear combination of
other vectors. In an other word, this vector does not contain useful information.

If equation \eqref{eqn:linear dependent} hold if and only if $ c_1 = c_2 = ... = c_{K} =
0 $, we say $ \left\{ \bm{a_1}, \bm{a_2}, ... \bm{a_{K}}\right\}  $ is 
{\textbf {linearly independent}}.




\section{Quadratic Form and Distance}
For a column vector $ \bm{x} = (x_1 x_2 ... x_{n})' $, we can use the square of 
Euclidean distance to measure the distance between column vector $ \bm{x} $ and 
origin $ O $ (zero vector). And it can be written in the form of dot product:

\begin{equation*}
\bm{x}'\bm{x} = (x_1 x_2 ... x_{n})(x_1 x_2 ... x_{n})' = \sum\limits_{i = 1} ^n x_{i}^{2}
\end{equation*}
Or we can write in this way
\begin{equation*}
\bm{x}'\bm{I}\bm{x} = \bm{x}'\bm{x}
\end{equation*}

It means we give the same weight ($ \bm{I} $) to each element in this dot product. 







\bibliographystyle{plainnat}
\bibliography{my_bib}

\end{document}

